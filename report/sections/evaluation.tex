\subsection{Evaluation}

For evaluating the models mentioned in \autoref{sec:training}, the following metrics were measured based the performance of the model on the testing dataset. In these definitions, the term \textit{Error} refers to the calculated difference between a prediction given by a model and ground truth given by the test dataset:

\def\RMSE{\textit{RMSE}\xspace}
\def\MAE{\textit{MAE}\xspace}
\def\MAPE{\textit{MAPE}\xspace}
\def\MAX{\textit{MAX}\xspace}
\def\RT{\textit{R2}\xspace}

\begin{itemize}
    \item \textit{Root Mean Squared Error} (RMSE) is based on squaring each error, average these error and then take the square root of this average. Since all errors are squared it highlights the largest errors, which in addition highlights models with a large errors. (\cite{hodson_root-mean-square_2022})
    \item \textit{Mean Absolute Error} (MAE) is based on taking the average of the absolute value of each error. This metrics is a simple metrics that gives a baseline for how well a model performs in general. (\cite{hodson_root-mean-square_2022})
    \item \textit{Mean Absolute Percentage Error} (MAPE) is based on taking the absolute error, which are then divided by the ground truth and averaged as a percentage. It shows how large errors are in respect to the ground truth of a given prediction. This means, that the metrics becomes unusable if the ground truth are zero or near zero.(\cite{myttenaere_mean_2016})
    \item \textit{Max Error} (MA) is a very simple metric that shows what the single largest error is. This metric is very useful along with RMSE, since it gives the actual largest error that a given model had.
    \item \textit{$R^2$} (\RT) measures how much of the variance in the target variable is explained by a regression model, expressed as a proportion between 0 and 1 (or negative if the model performs worse than predicting the mean). Unlike the other metrics included, which quantify the average size of prediction errors in the original units of the target, \RT is unitless and evaluates how well the model captures underlying variation rather than how large the errors are. Thus, \RMSE and \MAE measure error magnitude, while \RT measures explanatory power.(\cite{chicco_coefficient_2021})
\end{itemize}

Other metrics were collected and tested, but the mentioned metrics were deemed the most useful to achieve a model capable of solving the problem described in \autoref{sec:case}.

