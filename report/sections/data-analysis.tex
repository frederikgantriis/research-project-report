
\section{Data Analysis}

This project uses data given by \ap. All data is collected from the preheater tower mentioned in \autoref{sec:introduction}. This includes the following datapoints:

\begin{itemize}
    \item \texttt{PV1\_A\_Feed\_to\_Oven\_tons\_per\_day}:
    \item \texttt{PV2\_A\_Bottom\_Pressure}:
    \item \texttt{PV3\_A\_Bottom\_Temperature}:
    \item \texttt{PV4\_A\_Top\_Pressure}:
    \item \texttt{PV5\_A\_First\_Step\_Bottom\_Temperature}:
    \item \texttt{PV6\_A\_Second\_Step\_Bottom\_Temperature}:
    \item \texttt{PV7\_A\_Second\_Step\_Top\_Temperature}:
    \item \texttt{PV8\_A\_FlueGasFan\_Power}:
    \item \texttt{PV9\_A\_FlueGasFan\_Current}:
    \item \texttt{month}:
    \item \texttt{day}:
\end{itemize}

All of these values were then implemented twice: Once for the A side and once for the B side of the pre-heater tower.

In addition, two features were compute from the previous mentioned features:

\begin{itemize}
    \item \texttt{pressure\_diff}:
    \item \texttt{temperature\_diff}:
\end{itemize}

These two features were added, based on expert knowledge from \ap.

\section{Descriptive Statistics}

Before pre-processing and feature engineering, an explorative data analysis is needed to get a sense of which features would be deemed useful for the model, and how these features should be processed.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{report/img/descriptive_stats.png}
    \caption{Overview of Mean and std deviation of each feature}
    \label{fig:descriptive_stats}
\end{figure}

\autoref{fig:descriptive_stats} gives a quick overview of the mean values of each feature. This illustration shows that, while temperature readings are quite high, all other features generally deviate in a smaller range. However, due to the high temperature readings, scaling the input would be critical to enable good results from the models.

\subsection{Correlation Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{report/img/pearson_correlation_matrix.png}
    \caption{Pearson Correlation Matrix}
    \label{fig:pearson}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{report/img/spearman_correlation_matrix.png}
    \caption{Spearman Correlation Matrix}
    \label{fig:spearman}
\end{figure}

\todo{Write description of correlation analysis}

\subsection{Pre-processing and Feature Engineering}

\todo{Explain the steps of simpleimputer and so on}

\subsection{Train-Validation-Test Splitting Strategy}

In regards to splitting the dataset into a train, validation and test dataset, the main focus has been to keep the order of the data throughout all processing. The reason being, that time-series data may include patterns present over a given time-period that models are able to learn. Therefore, the initial splitting only concerned splitting the dataset into train and test. The size of the training dataset was determined using try-and-test experimentation to choose an optimal size.

Afterwards, the training dataset are split into different folds to enable cross-validation. The data is split using \textit{TimeSeriesSplit} \footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html}}. The method splits the data into equal portions, before being trained on the model using cross-validation. The reason for using the \textit{TimeSeriesSplit}, is to ensure that the data isn't shuffled, since all entries in a given split to be a continuous sequence. This ensures that the model can use patterns over a given time-period to predict future outcomes.

\subsection{Specific transformations}